{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "END_Assignment11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aEqiUhjzGdn"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qfb0k_Rs2SL"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "\r\n",
        "import spacy\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import random\r\n",
        "import math\r\n",
        "import time"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ljyZotLy1Mz"
      },
      "source": [
        "SEED = 1235\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtTathFky5Vg",
        "outputId": "334cccf2-ed06-4f2d-f5e7-93e633c79eda"
      },
      "source": [
        "!python -m spacy download en\r\n",
        "!python -m spacy download de"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJFm-YpR5rXl"
      },
      "source": [
        "spacy_de = spacy.load('de')\r\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_15ZRMm1Yol"
      },
      "source": [
        "def tokenize_de(text):\r\n",
        "    #tokenizes german text string, outputs a list of string\r\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\r\n",
        "\r\n",
        "def tokenize_en(text):\r\n",
        "    #tokenizes english text string, outputs a list of string\r\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qrk9wBP43XB"
      },
      "source": [
        "\r\n",
        "Initializing Fields to defive the processing. Unline RNNs, CNNs expect the batch to be in first dimension. Hence setting batch_first = True. We lower case all text and pad init and eos tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZY3BPOU4xNW"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_de, \r\n",
        "            init_token = '<sos>', \r\n",
        "            eos_token = '<eos>', \r\n",
        "            lower = True, \r\n",
        "            batch_first = True)\r\n",
        "\r\n",
        "TRG = Field(tokenize = tokenize_en, \r\n",
        "            init_token = '<sos>', \r\n",
        "            eos_token = '<eos>', \r\n",
        "            lower = True, \r\n",
        "            batch_first = True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNmRo7To5cYX"
      },
      "source": [
        "Dividing dataset into train, valid and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35yVy7EI5Xb-"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), \r\n",
        "                                                    fields=(SRC, TRG))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWJg5igr5xaX"
      },
      "source": [
        "Build arc and trg vocabulary as before. Tokens which appear less than 2 times are converted into <unk> tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyp3gLyT5xFO"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\r\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdfRmQsw6EXH"
      },
      "source": [
        "define device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEmQRsGU6FvZ"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnvVN5Lz6Ll5"
      },
      "source": [
        "Define train, valid and test iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9tqNaFw6LFU"
      },
      "source": [
        "BATCH_SIZE = 128\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data), \r\n",
        "     batch_size = BATCH_SIZE,\r\n",
        "     device = device)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgI8CfGx6cSl"
      },
      "source": [
        "#Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AWHJ_WX6gDC"
      },
      "source": [
        "Encoder in the convolutional sequence-to-sequence model is a little different - gives two context vectors for each token in the input sentence. If input sentence had n tokens, we would get 2*n context vectors, two for each token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu5dTIcV7HJV"
      },
      "source": [
        "Steps:\r\n",
        "\r\n",
        "*  Token is passed through a token embedding layer\r\n",
        "*  To have the sense for tokens order, positional embedding layer is introduced which takes the position of the token within the sequence ranging [0,n) where n is length os sequence.\r\n",
        "*  We take element wise sum of token and positional embeddings - call this as embedding vector\r\n",
        "* This embedding vector is passed through a linear layer and outputs a vector with the required hidden dimension size. - call this as hidden vector\r\n",
        "* This hidden vector into $N$ convolutional blocks. \r\n",
        "* Output of convolution blocks is passed through a linear layer and tranforms to embedding dimension size - call this as our conved vector \r\n",
        "* We take element wise sum of conved vector and embedding vector via a residual connection - call this as combined vector\r\n",
        "* For each token, now we have conved vector and combined vector "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTp8Dqk36aNx"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 input_dim, \r\n",
        "                 emb_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 kernel_size, \r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 100):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        # checking kernel size to be odd\r\n",
        "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\r\n",
        "        \r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device) # useful to scale the combined vectors\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\r\n",
        "        \r\n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\r\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \r\n",
        "                                              out_channels = 2 * hid_dim, \r\n",
        "                                              kernel_size = kernel_size, \r\n",
        "                                              padding = (kernel_size - 1) // 2)\r\n",
        "                                    for _ in range(n_layers)]) # sequence of conv layers \r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src):\r\n",
        "        \r\n",
        "        #src = [batch_size,src_len]\r\n",
        "        \r\n",
        "        batch_size = src.shape[0]\r\n",
        "        src_len = src.shape[1]\r\n",
        "  \r\n",
        "        #position tensor\r\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "\r\n",
        "        #pos = [batch_size,src_len]\r\n",
        "        \r\n",
        "        tok_embedded = self.tok_embedding(src) #embed tokens\r\n",
        "        pos_embedded = self.pos_embedding(pos) #position tokens\r\n",
        "        \r\n",
        "        #tok_embedded =  [batch_size,src_len,emb_dim]\r\n",
        "        #pos_embedded = [batch_size,src_len,emb_dim]\r\n",
        "        #elementwise summ\r\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\r\n",
        "        \r\n",
        "        #embedded = [batch_size, src_len, emb_dim]\r\n",
        "        \r\n",
        "        #input to conv\r\n",
        "        conv_input = self.emb2hid(embedded)\r\n",
        "        \r\n",
        "        #conv_input = [batch_size, src_len, hid_dim]\r\n",
        "        \r\n",
        "        #permute as CNNs require src_len at last\r\n",
        "        conv_input = conv_input.permute(0, 2, 1) \r\n",
        "        \r\n",
        "        #conv_input = [batch_size,hid_dim,src_len]\r\n",
        "        \r\n",
        "        \r\n",
        "        for i, conv in enumerate(self.convs):\r\n",
        "        \r\n",
        "            conved = conv(self.dropout(conv_input))\r\n",
        "\r\n",
        "            #conved = [batch_size,2*hid_dim,src_len] \r\n",
        "\r\n",
        "            #GLU activation function halves the dimension\r\n",
        "            conved = F.glu(conved, dim = 1)\r\n",
        "\r\n",
        "            #conved = [batch_size,hid_dim,src_len] \r\n",
        "            \r\n",
        "            #element wise sum - residual connection\r\n",
        "            conved = (conved + conv_input) * self.scale\r\n",
        "\r\n",
        "            #conved = [batch_size,hid_dim,src_len] \r\n",
        "            \r\n",
        "\r\n",
        "            conv_input = conved\r\n",
        "        \r\n",
        "        \r\n",
        "        #permute back to linear layer dim format\r\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\r\n",
        "        \r\n",
        "        #conved = [batch_size,src_len,emb_dim]\r\n",
        "        \r\n",
        "        #elementwise sum output (conved) and input (embedded) \r\n",
        "        combined = (conved + embedded) * self.scale\r\n",
        "        \r\n",
        "        #combined = [batch_size,src_len,emb_dim]\r\n",
        "      \r\n",
        "        \r\n",
        "        return conved, combined # return two vectors for each token"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18rei83xCrA5"
      },
      "source": [
        "#Decoder\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8nqzgqTCtU5"
      },
      "source": [
        "Decoder takes the groundtruth sentence and tries to predict it. Differences to Encoder are\r\n",
        "\r\n",
        "\r\n",
        "*  Embeddings are fed into the convolutional blocks inatead of a residual connection that connects after the convolutional blocks and the transformation. \r\n",
        "\r\n",
        "* Encoder conved and combined outputs are used as inputs to convolutional blocks.\r\n",
        "\r\n",
        "* Decoder output is a linear layer from embedding dimension to output dimension.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhxKRF44DhrT"
      },
      "source": [
        "Convolution block is as follows:\r\n",
        "*  Padding is done only at the beginning of the sentence. This would avoid the model from looking at the next tokens. As in saves from cheating.\r\n",
        "*   Attentuion is applied after GLU activation and before the residual connection\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITZ2d4WqCsy2"
      },
      "source": [
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 output_dim, \r\n",
        "                 emb_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 kernel_size, \r\n",
        "                 dropout, \r\n",
        "                 trg_pad_idx, \r\n",
        "                 device,\r\n",
        "                 max_length = 100):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.kernel_size = kernel_size\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)# useful to scale the combined vectors\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\r\n",
        "        \r\n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\r\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\r\n",
        "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\r\n",
        "        \r\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \r\n",
        "                                              out_channels = 2 * hid_dim, \r\n",
        "                                              kernel_size = kernel_size)\r\n",
        "                                    for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "      \r\n",
        "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\r\n",
        "        \r\n",
        "        #embedded = [batch_size, trg_len, emb_dim]\r\n",
        "        #conved = [batch_size, hid_dim, trg_len]\r\n",
        "        #encoder_conved = [batch_size, src_len, emb_dim]\r\n",
        "        #encoder_combined = [batch_size, src_len, emb_dim]\r\n",
        "        \r\n",
        "        #permute back to linear layer dim format\r\n",
        "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\r\n",
        "        \r\n",
        "        #conved_emb =[batch_size, trg_len, emb_dim]\r\n",
        "        \r\n",
        "        combined = (conved_emb + embedded) * self.scale\r\n",
        "        \r\n",
        "        #combined =[batch_size, trg_len, emb_dim]\r\n",
        "                \r\n",
        "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\r\n",
        "        \r\n",
        "        #energy = [batch_size, trg_len, src_len]\r\n",
        "        \r\n",
        "        attention = F.softmax(energy, dim=2)\r\n",
        "        \r\n",
        "        #attention = [batch_size, trg_len, src_dim]\r\n",
        "            \r\n",
        "        attended_encoding = torch.matmul(attention, encoder_combined)\r\n",
        "        \r\n",
        "        #attended_encoding = [batch_size, trg_len, emb_dim]\r\n",
        "        \r\n",
        "        \r\n",
        "        attended_encoding = self.attn_emb2hid(attended_encoding)\r\n",
        "        \r\n",
        "        #attended_encoding = [batch_size, trg_len, hid_dim]\r\n",
        "        \r\n",
        "        #element wise sum and residual connection\r\n",
        "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\r\n",
        "        \r\n",
        "        #attended_combined = [batch_size, hid_dim, trg_len]\r\n",
        "        \r\n",
        "        return attention, attended_combined\r\n",
        "        \r\n",
        "    def forward(self, trg, encoder_conved, encoder_combined):\r\n",
        "        \r\n",
        "        #trg = [batch_size, trg_len]\r\n",
        "                \r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "            \r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [batch_size, trg_len]\r\n",
        "        \r\n",
        "        #embed tokens and positions\r\n",
        "        tok_embedded = self.tok_embedding(trg)\r\n",
        "        pos_embedded = self.pos_embedding(pos)\r\n",
        "        \r\n",
        "        #tok_embedded =  [batch_size,trg_len,emb_dim]\r\n",
        "        #pos_embedded = [batch_size,trg_len,emb_dim]\r\n",
        "        \r\n",
        "        #element wise sum of tok and pos embeddings \r\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\r\n",
        "        \r\n",
        "        #embedded = [batch_size, trg_len, emb_dim]\r\n",
        "        \r\n",
        " \r\n",
        "        conv_input = self.emb2hid(embedded)\r\n",
        "        \r\n",
        "        #conv_input = [batch_size, trg_len, hid_dim]\r\n",
        "        \r\n",
        "         #permute as CNNs require trg_len at last\r\n",
        "        conv_input = conv_input.permute(0, 2, 1) \r\n",
        "        \r\n",
        "        #conv_input = [batch_size, hid_dim, trg_len]\r\n",
        "        \r\n",
        "        batch_size = conv_input.shape[0]\r\n",
        "        hid_dim = conv_input.shape[1]\r\n",
        "        \r\n",
        "        for i, conv in enumerate(self.convs):\r\n",
        "        \r\n",
        "            #apply dropout\r\n",
        "            conv_input = self.dropout(conv_input)\r\n",
        "        \r\n",
        "            #pad at beginning\r\n",
        "            padding = torch.zeros(batch_size, \r\n",
        "                                  hid_dim, \r\n",
        "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\r\n",
        "                \r\n",
        "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\r\n",
        "        \r\n",
        "            #padded_conv_input = [batch_size, hid_dim, trg_len + kernel_size - 1]\r\n",
        "\r\n",
        "            conved = conv(padded_conv_input)\r\n",
        "\r\n",
        "            #conved = [batch_size, 2 * hid_dim, trg_len]\r\n",
        "            \r\n",
        "            #GLU activation function halves the dimension\r\n",
        "            conved = F.glu(conved, dim = 1)\r\n",
        "\r\n",
        "            #conved = [batch_size, hid_dim, trg_len]\r\n",
        "            \r\n",
        "            # attention\r\n",
        "            attention, conved = self.calculate_attention(embedded, \r\n",
        "                                                         conved, \r\n",
        "                                                         encoder_conved, \r\n",
        "                                                         encoder_combined)\r\n",
        "            \r\n",
        "            #attention = [batch_size, trg_len, src_len]\r\n",
        "            \r\n",
        "            #element wise sum, residual connection\r\n",
        "            conved = (conved + conv_input) * self.scale\r\n",
        "            \r\n",
        "            #conved = [batch_size, hid_dim, trg_len]\r\n",
        "            \r\n",
        "         \r\n",
        "            conv_input = conved\r\n",
        "        #permute back to linear layer dim format   \r\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\r\n",
        "         \r\n",
        "        #conved = [batch_size, trg_len, emb_dim]\r\n",
        "            \r\n",
        "        output = self.fc_out(self.dropout(conved))\r\n",
        "        \r\n",
        "        #output = [batch_size, trg_len, output_dim]\r\n",
        "            \r\n",
        "        return output, attention"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH_ADHp9HCAO"
      },
      "source": [
        "#Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKC71lBJHBe6"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        \r\n",
        "    def forward(self, src, trg):\r\n",
        "        \r\n",
        "        #src = [batch_size, src_len]\r\n",
        "        #trg = [batch_size, trg_len - 1] (no <eos> token at the end)\r\n",
        "           \r\n",
        "        encoder_conved, encoder_combined = self.encoder(src)\r\n",
        "            \r\n",
        "        #encoder_conved = [batch_size, sr_len, emb_dim]\r\n",
        "        #encoder_combined = [batch_size, src_len, emb_dim]\r\n",
        "        \r\n",
        "\r\n",
        "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\r\n",
        "        \r\n",
        "        #output = [batch_size, trg_len - 1, output_dim]\r\n",
        "        #attention = [batch_size, trg_len - 1, src_len]\r\n",
        "        \r\n",
        "        return output, attention"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z47oTtnoHfob"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWcpKZP6HhJO"
      },
      "source": [
        "\r\n",
        "INPUT_DIM = len(SRC.vocab)\r\n",
        "OUTPUT_DIM = len(TRG.vocab)\r\n",
        "EMB_DIM = 256\r\n",
        "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\r\n",
        "ENC_LAYERS = 10 # number of conv. blocks in encoder\r\n",
        "DEC_LAYERS = 10 # number of conv. blocks in decoder\r\n",
        "ENC_KERNEL_SIZE = 3 # must be odd!\r\n",
        "DEC_KERNEL_SIZE = 3 # can be even or odd\r\n",
        "ENC_DROPOUT = 0.25\r\n",
        "DEC_DROPOUT = 0.25\r\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n",
        "    \r\n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\r\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec).to(device)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0-GAuiGHnta"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRebhck4HzBp"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src = batch.src\r\n",
        "        trg = batch.trg\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "        \r\n",
        "        #output = [batch_size, trg_len - 1, output_dim]\r\n",
        "        #trg = [batch_size, trg_len]\r\n",
        "        \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "        \r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "        \r\n",
        "        #output = [batch_size * trg_len - 1, output_dim]\r\n",
        "        #trg = [batch_size * trg_len - 1]\r\n",
        "        \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giZE3QX2H9Gv"
      },
      "source": [
        "\r\n",
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src = batch.src\r\n",
        "            trg = batch.trg\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "        \r\n",
        "            #output = [batch_size, trg_len - 1, output_dim]\r\n",
        "            #trg = [batch_size, trg_len]\r\n",
        "\r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "\r\n",
        "            #output = [batch_size * trg_len - 1, output_dim]\r\n",
        "            #trg = [batch_size * trg_len - 1]\r\n",
        "            \r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb6ZvXieIE2b"
      },
      "source": [
        "\r\n",
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du6a-ISfIHbq",
        "outputId": "27905a19-0fc8-4ad3-8978-0fe6c76415ec"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "CLIP = 0.1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 1m 6s\n",
            "\tTrain Loss: 4.193 | Train PPL:  66.233\n",
            "\t Val. Loss: 2.878 |  Val. PPL:  17.779\n",
            "Epoch: 02 | Time: 1m 6s\n",
            "\tTrain Loss: 2.993 | Train PPL:  19.951\n",
            "\t Val. Loss: 2.335 |  Val. PPL:  10.329\n",
            "Epoch: 03 | Time: 1m 5s\n",
            "\tTrain Loss: 2.590 | Train PPL:  13.331\n",
            "\t Val. Loss: 2.121 |  Val. PPL:   8.336\n",
            "Epoch: 04 | Time: 1m 5s\n",
            "\tTrain Loss: 2.370 | Train PPL:  10.697\n",
            "\t Val. Loss: 1.993 |  Val. PPL:   7.336\n",
            "Epoch: 05 | Time: 1m 5s\n",
            "\tTrain Loss: 2.222 | Train PPL:   9.228\n",
            "\t Val. Loss: 1.904 |  Val. PPL:   6.712\n",
            "Epoch: 06 | Time: 1m 5s\n",
            "\tTrain Loss: 2.111 | Train PPL:   8.260\n",
            "\t Val. Loss: 1.861 |  Val. PPL:   6.433\n",
            "Epoch: 07 | Time: 1m 5s\n",
            "\tTrain Loss: 2.030 | Train PPL:   7.612\n",
            "\t Val. Loss: 1.827 |  Val. PPL:   6.217\n",
            "Epoch: 08 | Time: 1m 6s\n",
            "\tTrain Loss: 1.957 | Train PPL:   7.080\n",
            "\t Val. Loss: 1.806 |  Val. PPL:   6.088\n",
            "Epoch: 09 | Time: 1m 5s\n",
            "\tTrain Loss: 1.903 | Train PPL:   6.709\n",
            "\t Val. Loss: 1.749 |  Val. PPL:   5.746\n",
            "Epoch: 10 | Time: 1m 6s\n",
            "\tTrain Loss: 1.856 | Train PPL:   6.399\n",
            "\t Val. Loss: 1.744 |  Val. PPL:   5.722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skSYGodkMN2u",
        "outputId": "2e310438-114c-44f0-b9f2-80a554bdbfad"
      },
      "source": [
        "\r\n",
        "model.load_state_dict(torch.load('tut5-model.pt'))\r\n",
        "\r\n",
        "test_loss = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 1.807 | Test PPL:   6.095 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSOizBKZMTJ4"
      },
      "source": [
        "#Inference Steps\r\n",
        "\r\n",
        "\r\n",
        "*   tokenize the source sentence\r\n",
        "*   append the <sos> and <eos> tokens\r\n",
        "*   generate source indices, convert to tensor and unsqueeze it to get into right format\r\n",
        "* forward source sentence into the encoder\r\n",
        "* create a list to have the output sentence, initialized with  <sos> token\r\n",
        "* Run a while loop till we have not hit a maximum length:\r\n",
        "\r\n",
        "  *   convert the current prediction to a tensor and unsqueeze it to get into right format\r\n",
        "  *   Foeward the current prediction, two encoder outputs into the decoder\r\n",
        "  *   Get next token prediction from decoder and add prediction to current output sentence \r\n",
        "  *   break if the prediction comes <eos> token\r\n",
        "  *   convert the output sentence from indexes to tokens\r\n",
        "  *   return the output sentence and attention from final layer\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6uW7B5lMRSC"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "        \r\n",
        "    if isinstance(sentence, str):\r\n",
        "        nlp = spacy.load('de')\r\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\r\n",
        "    else:\r\n",
        "        tokens = [token.lower() for token in sentence]\r\n",
        "\r\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\r\n",
        "        \r\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        encoder_conved, encoder_combined = model.encoder(src_tensor)\r\n",
        "\r\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n",
        "\r\n",
        "    for i in range(max_len):\r\n",
        "\r\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\r\n",
        "        \r\n",
        "        pred_token = output.argmax(2)[:,-1].item()\r\n",
        "        \r\n",
        "        trg_indexes.append(pred_token)\r\n",
        "\r\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n",
        "            break\r\n",
        "    \r\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n",
        "    \r\n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDoMhM2mQy2Y",
        "outputId": "1b1fe983-db51-4b52-e68d-13eea62fec35"
      },
      "source": [
        "example_idx = 77\r\n",
        "\r\n",
        "src = vars(test_data.examples[example_idx])['src']\r\n",
        "trg = vars(test_data.examples[example_idx])['trg']\r\n",
        "\r\n",
        "print(f'src = {src}')\r\n",
        "print(f'trg = {trg}')\r\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n",
        "\r\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['ein', 'brauner', 'hund', 'läuft', 'durchs', 'gras', 'und', 'seine', 'zunge', 'hängt', 'heraus', '.']\n",
            "trg = ['a', 'brown', 'dog', 'walks', 'in', 'the', 'grass', 'with', 'its', 'tongue', 'hanging', 'out', '.']\n",
            "predicted trg = ['a', 'brown', 'dog', 'runs', 'through', 'the', 'grass', 'with', 'his', 'tongue', 'hanging', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}