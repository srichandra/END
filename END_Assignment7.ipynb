{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "END_Assignment7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYiRsFGD6iUC"
      },
      "source": [
        "# 0 TorchText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp5IzBGsPGHs"
      },
      "source": [
        "## Dataset Preview\n",
        "\n",
        "Your first step to deep learning in NLP. We will be mostly using PyTorch. Just like torchvision, PyTorch provides an official library, torchtext, for handling text-processing pipelines. \n",
        "\n",
        "We will be using previous session tweet dataset. Let's just preview the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1-Yz-5RRFYc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye3uW-wZTgHP"
      },
      "source": [
        "phrases_data = pd.read_csv('/content/sample_data/dictionary.txt',sep='|',header=None).sort_values(by=1).rename(columns={0:'phrase',1:'id'})\r\n",
        "label_data = pd.read_csv('/content/sample_data/sentiment_labels.txt',sep='|').rename(columns={'sentiment values':'sentiment'}).rename(columns={'phrase ids':'id'})\r\n",
        "assert phrases_data.shape[0] == label_data.shape[0]  \r\n",
        "joined_data = pd.merge(phrases_data,label_data,on='id')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQWuYkrkEAQ4"
      },
      "source": [
        "label_dict = dict(zip(joined_data['phrase'],joined_data['sentiment']))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0M9ja2cAOIv"
      },
      "source": [
        "labels = []\r\n",
        "with open('/content/sample_data/SOStr.txt','r') as f:\r\n",
        "    count=0\r\n",
        "    for line in f.readlines():\r\n",
        "        temp = line.strip().split('|')\r\n",
        "        value = [label_dict.get(c) for c in temp]\r\n",
        "        labels.append(value)    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7JdpCW-YbAG"
      },
      "source": [
        "df = pd.read_csv('/content/sample_data/datasetSentences.txt',sep='\\t')\r\n",
        "df.head()\r\n",
        "df['label'] = labels"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqRsoF6xYdgl"
      },
      "source": [
        "df['label'] = df['label'].apply(lambda x: np.ceil(25*np.mean(x))).astype('int')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pbMO0V6Y0KI"
      },
      "source": [
        "label_map=dict(zip(list(df.label.unique()),range(len(df.label.unique()))))\r\n",
        "df['label']=df['label'].map(label_map)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTmwNb1mgZjx"
      },
      "source": [
        "datasplit = pd.read_csv('/content/sample_data/datasetSplit.txt')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecAuU7OrggEL"
      },
      "source": [
        "df['split'] = datasplit['splitset_label']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CLMXvKAgizL"
      },
      "source": [
        "train_data = df[df['split']==1]\r\n",
        "valid_data = df[df['split']!=1]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RjCkYscg3By"
      },
      "source": [
        "def random_deletion(words, p=0.5): \r\n",
        "    if len(words) == 1: # return if single word\r\n",
        "        return words\r\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \r\n",
        "    if len(remaining) == 0: # if not left, sample a random word\r\n",
        "        return [random.choice(words)] \r\n",
        "    else:\r\n",
        "        return remaining"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZEa-v12hFHn"
      },
      "source": [
        "def random_swap(sentence, n=5): \r\n",
        "    length = range(len(sentence)) \r\n",
        "    for _ in range(n):\r\n",
        "        idx1, idx2 = random.sample(length, 2)\r\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \r\n",
        "    return \" \".join(i for i in sentence)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvZ7jZPehLWd",
        "outputId": "5f789e52-d5a3-49e1-d875-d07a7da33871"
      },
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.6/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.6/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.11.21)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.11.8)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.4.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: contextvars>=2.1; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from sniffio->httpx==0.13.3->googletrans==3.1.0a0) (2.4)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.6/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars>=2.1; python_version < \"3.7\"->sniffio->httpx==0.13.3->googletrans==3.1.0a0) (0.14)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.6/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06UzA-H2ha6c"
      },
      "source": [
        "import random\r\n",
        "import googletrans\r\n",
        "from googletrans import Translator\r\n",
        "def random_translation(sentence,translator=Translator()):\r\n",
        "    available_langs = list(googletrans.LANGUAGES.keys()) \r\n",
        "    trans_lang = random.choice(available_langs) \r\n",
        "\r\n",
        "\r\n",
        "    translations = translator.translate(sentence, dest=trans_lang) \r\n",
        "    t_text = [t.text for t in translations]\r\n",
        "\r\n",
        "\r\n",
        "    translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \r\n",
        "    en_text = \" \".join(t.text for t in translations_en_random)\r\n",
        "    return en_text"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6gYP5Ph6izQ"
      },
      "source": [
        "adding random deletions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToSM1tS8iHwM"
      },
      "source": [
        "# add random deletions\r\n",
        "aug_data = pd.DataFrame()\r\n",
        "for i,row in train_data.iterrows():\r\n",
        "    row['sentence'] = ' '.join(random_deletion(row['sentence'].split(), p=0.2))\r\n",
        "    label = row['label']\r\n",
        "    aug_data = aug_data.append(row).reset_index(drop=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek7371nT6O0E"
      },
      "source": [
        "translated for only 2600 samples for timebeing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zn_ao93-hIH_",
        "outputId": "53c0db6c-2208-42cc-c067-07b963016ebf"
      },
      "source": [
        "# add random translations\r\n",
        "aug_data1 = pd.DataFrame()\r\n",
        "for i,row in train_data.iterrows():\r\n",
        "    print(i)\r\n",
        "    row['sentence'] = random_translation([str(row['sentence'])]) \r\n",
        "    label = row['label']\r\n",
        "    aug_data1 = aug_data1.append(row).reset_index(drop=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "67\n",
            "71\n",
            "81\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "213\n",
            "227\n",
            "339\n",
            "382\n",
            "386\n",
            "387\n",
            "427\n",
            "428\n",
            "445\n",
            "466\n",
            "467\n",
            "472\n",
            "473\n",
            "485\n",
            "486\n",
            "487\n",
            "513\n",
            "583\n",
            "669\n",
            "691\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "741\n",
            "747\n",
            "748\n",
            "839\n",
            "892\n",
            "893\n",
            "895\n",
            "896\n",
            "1111\n",
            "1152\n",
            "1153\n",
            "1214\n",
            "1240\n",
            "1255\n",
            "1273\n",
            "1481\n",
            "1488\n",
            "1587\n",
            "1670\n",
            "1671\n",
            "1672\n",
            "1673\n",
            "1674\n",
            "1675\n",
            "1676\n",
            "1677\n",
            "1678\n",
            "1679\n",
            "1680\n",
            "1681\n",
            "1682\n",
            "1683\n",
            "1684\n",
            "1685\n",
            "1686\n",
            "1687\n",
            "1688\n",
            "1689\n",
            "1690\n",
            "1691\n",
            "1692\n",
            "1693\n",
            "1694\n",
            "1695\n",
            "1696\n",
            "1697\n",
            "1698\n",
            "1699\n",
            "1700\n",
            "1701\n",
            "1702\n",
            "1703\n",
            "1704\n",
            "1705\n",
            "1706\n",
            "1707\n",
            "1708\n",
            "1709\n",
            "1710\n",
            "1711\n",
            "1712\n",
            "1713\n",
            "1714\n",
            "1715\n",
            "1716\n",
            "1717\n",
            "1718\n",
            "1719\n",
            "1720\n",
            "1721\n",
            "1722\n",
            "1723\n",
            "1724\n",
            "1725\n",
            "1726\n",
            "1727\n",
            "1728\n",
            "1729\n",
            "1730\n",
            "1731\n",
            "1732\n",
            "1733\n",
            "1734\n",
            "1735\n",
            "1736\n",
            "1737\n",
            "1738\n",
            "1739\n",
            "1740\n",
            "1741\n",
            "1742\n",
            "1743\n",
            "1744\n",
            "1745\n",
            "1746\n",
            "1747\n",
            "1748\n",
            "1749\n",
            "1750\n",
            "1751\n",
            "1752\n",
            "1753\n",
            "1754\n",
            "1755\n",
            "1756\n",
            "1757\n",
            "1758\n",
            "1759\n",
            "1760\n",
            "1761\n",
            "1762\n",
            "1763\n",
            "1764\n",
            "1765\n",
            "1766\n",
            "1767\n",
            "1768\n",
            "1769\n",
            "1770\n",
            "1771\n",
            "1772\n",
            "1773\n",
            "1774\n",
            "1775\n",
            "1776\n",
            "1777\n",
            "1778\n",
            "1779\n",
            "1780\n",
            "1781\n",
            "1782\n",
            "1783\n",
            "1784\n",
            "1785\n",
            "1786\n",
            "1787\n",
            "1788\n",
            "1789\n",
            "1790\n",
            "1791\n",
            "1792\n",
            "1793\n",
            "1794\n",
            "1795\n",
            "1796\n",
            "1797\n",
            "1798\n",
            "1799\n",
            "1800\n",
            "1801\n",
            "1802\n",
            "1803\n",
            "1804\n",
            "1805\n",
            "1806\n",
            "1807\n",
            "1808\n",
            "1809\n",
            "1810\n",
            "1811\n",
            "1812\n",
            "1813\n",
            "1814\n",
            "1815\n",
            "1816\n",
            "1817\n",
            "1818\n",
            "1819\n",
            "1820\n",
            "1821\n",
            "1822\n",
            "1823\n",
            "1824\n",
            "1825\n",
            "1826\n",
            "1827\n",
            "1828\n",
            "1829\n",
            "1830\n",
            "1831\n",
            "1832\n",
            "1833\n",
            "1834\n",
            "1835\n",
            "1836\n",
            "1837\n",
            "1838\n",
            "1839\n",
            "1840\n",
            "1841\n",
            "1842\n",
            "1843\n",
            "1844\n",
            "1845\n",
            "1846\n",
            "1847\n",
            "1848\n",
            "1849\n",
            "1850\n",
            "1851\n",
            "1852\n",
            "1853\n",
            "1854\n",
            "1855\n",
            "1856\n",
            "1857\n",
            "1858\n",
            "1859\n",
            "1860\n",
            "1861\n",
            "1862\n",
            "1863\n",
            "1864\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1868\n",
            "1869\n",
            "1870\n",
            "1871\n",
            "1872\n",
            "1873\n",
            "1874\n",
            "1875\n",
            "1876\n",
            "1877\n",
            "1878\n",
            "1879\n",
            "1880\n",
            "1881\n",
            "1882\n",
            "1883\n",
            "1884\n",
            "1885\n",
            "1886\n",
            "1887\n",
            "1888\n",
            "1889\n",
            "1890\n",
            "1891\n",
            "1892\n",
            "1893\n",
            "1894\n",
            "1895\n",
            "1896\n",
            "1897\n",
            "1898\n",
            "1899\n",
            "1900\n",
            "1901\n",
            "1902\n",
            "1903\n",
            "1904\n",
            "1905\n",
            "1906\n",
            "1907\n",
            "1908\n",
            "1909\n",
            "1910\n",
            "1911\n",
            "1912\n",
            "1913\n",
            "1914\n",
            "1915\n",
            "1916\n",
            "1917\n",
            "1918\n",
            "1919\n",
            "1920\n",
            "1921\n",
            "1922\n",
            "1923\n",
            "1924\n",
            "1925\n",
            "1926\n",
            "1927\n",
            "1928\n",
            "1929\n",
            "1930\n",
            "1931\n",
            "1932\n",
            "1933\n",
            "1934\n",
            "1935\n",
            "1936\n",
            "1937\n",
            "1938\n",
            "1939\n",
            "1940\n",
            "1941\n",
            "1942\n",
            "1943\n",
            "1944\n",
            "1945\n",
            "1946\n",
            "1947\n",
            "1948\n",
            "1949\n",
            "1950\n",
            "1951\n",
            "1952\n",
            "1953\n",
            "1954\n",
            "1955\n",
            "1956\n",
            "1957\n",
            "1958\n",
            "1959\n",
            "1960\n",
            "1961\n",
            "1962\n",
            "1963\n",
            "1964\n",
            "1965\n",
            "1966\n",
            "1967\n",
            "1968\n",
            "1969\n",
            "1970\n",
            "1971\n",
            "1972\n",
            "1973\n",
            "1974\n",
            "1975\n",
            "1976\n",
            "1977\n",
            "1978\n",
            "1979\n",
            "1980\n",
            "1981\n",
            "1982\n",
            "1983\n",
            "1984\n",
            "1985\n",
            "1986\n",
            "1987\n",
            "1988\n",
            "1989\n",
            "1990\n",
            "1991\n",
            "1992\n",
            "1993\n",
            "1994\n",
            "1995\n",
            "1996\n",
            "1997\n",
            "1998\n",
            "1999\n",
            "2000\n",
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "2025\n",
            "2026\n",
            "2027\n",
            "2028\n",
            "2029\n",
            "2030\n",
            "2031\n",
            "2032\n",
            "2033\n",
            "2034\n",
            "2035\n",
            "2036\n",
            "2037\n",
            "2038\n",
            "2039\n",
            "2040\n",
            "2041\n",
            "2042\n",
            "2043\n",
            "2044\n",
            "2045\n",
            "2046\n",
            "2047\n",
            "2048\n",
            "2049\n",
            "2050\n",
            "2051\n",
            "2052\n",
            "2053\n",
            "2054\n",
            "2055\n",
            "2056\n",
            "2057\n",
            "2058\n",
            "2059\n",
            "2060\n",
            "2061\n",
            "2062\n",
            "2063\n",
            "2064\n",
            "2065\n",
            "2066\n",
            "2067\n",
            "2068\n",
            "2069\n",
            "2070\n",
            "2071\n",
            "2072\n",
            "2073\n",
            "2074\n",
            "2075\n",
            "2076\n",
            "2077\n",
            "2078\n",
            "2079\n",
            "2080\n",
            "2081\n",
            "2082\n",
            "2083\n",
            "2084\n",
            "2085\n",
            "2086\n",
            "2087\n",
            "2088\n",
            "2089\n",
            "2090\n",
            "2091\n",
            "2092\n",
            "2093\n",
            "2094\n",
            "2095\n",
            "2096\n",
            "2097\n",
            "2098\n",
            "2099\n",
            "2100\n",
            "2101\n",
            "2102\n",
            "2103\n",
            "2104\n",
            "2105\n",
            "2106\n",
            "2107\n",
            "2108\n",
            "2109\n",
            "2110\n",
            "2111\n",
            "2112\n",
            "2113\n",
            "2114\n",
            "2115\n",
            "2116\n",
            "2117\n",
            "2118\n",
            "2119\n",
            "2120\n",
            "2121\n",
            "2122\n",
            "2123\n",
            "2124\n",
            "2125\n",
            "2126\n",
            "2127\n",
            "2128\n",
            "2129\n",
            "2130\n",
            "2131\n",
            "2132\n",
            "2133\n",
            "2134\n",
            "2135\n",
            "2136\n",
            "2137\n",
            "2138\n",
            "2139\n",
            "2140\n",
            "2141\n",
            "2142\n",
            "2143\n",
            "2144\n",
            "2145\n",
            "2146\n",
            "2147\n",
            "2148\n",
            "2149\n",
            "2150\n",
            "2151\n",
            "2152\n",
            "2153\n",
            "2154\n",
            "2155\n",
            "2156\n",
            "2157\n",
            "2158\n",
            "2159\n",
            "2160\n",
            "2161\n",
            "2162\n",
            "2163\n",
            "2164\n",
            "2165\n",
            "2166\n",
            "2167\n",
            "2168\n",
            "2169\n",
            "2170\n",
            "2171\n",
            "2172\n",
            "2173\n",
            "2174\n",
            "2175\n",
            "2176\n",
            "2177\n",
            "2178\n",
            "2179\n",
            "2180\n",
            "2181\n",
            "2182\n",
            "2183\n",
            "2184\n",
            "2185\n",
            "2186\n",
            "2187\n",
            "2188\n",
            "2189\n",
            "2190\n",
            "2191\n",
            "2192\n",
            "2193\n",
            "2194\n",
            "2195\n",
            "2196\n",
            "2197\n",
            "2198\n",
            "2199\n",
            "2200\n",
            "2201\n",
            "2202\n",
            "2203\n",
            "2204\n",
            "2205\n",
            "2206\n",
            "2207\n",
            "2208\n",
            "2209\n",
            "2210\n",
            "2211\n",
            "2212\n",
            "2213\n",
            "2214\n",
            "2215\n",
            "2216\n",
            "2217\n",
            "2218\n",
            "2219\n",
            "2220\n",
            "2221\n",
            "2222\n",
            "2223\n",
            "2224\n",
            "2225\n",
            "2226\n",
            "2227\n",
            "2228\n",
            "2229\n",
            "2230\n",
            "2231\n",
            "2232\n",
            "2233\n",
            "2234\n",
            "2235\n",
            "2236\n",
            "2237\n",
            "2238\n",
            "2239\n",
            "2240\n",
            "2241\n",
            "2242\n",
            "2243\n",
            "2244\n",
            "2245\n",
            "2246\n",
            "2247\n",
            "2248\n",
            "2249\n",
            "2250\n",
            "2251\n",
            "2252\n",
            "2253\n",
            "2254\n",
            "2255\n",
            "2256\n",
            "2257\n",
            "2258\n",
            "2259\n",
            "2260\n",
            "2261\n",
            "2262\n",
            "2263\n",
            "2264\n",
            "2265\n",
            "2266\n",
            "2267\n",
            "2268\n",
            "2269\n",
            "2270\n",
            "2271\n",
            "2272\n",
            "2273\n",
            "2274\n",
            "2275\n",
            "2276\n",
            "2277\n",
            "2278\n",
            "2279\n",
            "2280\n",
            "2281\n",
            "2282\n",
            "2283\n",
            "2284\n",
            "2285\n",
            "2286\n",
            "2287\n",
            "2288\n",
            "2289\n",
            "2290\n",
            "2291\n",
            "2292\n",
            "2293\n",
            "2294\n",
            "2295\n",
            "2296\n",
            "2297\n",
            "2298\n",
            "2299\n",
            "2300\n",
            "2301\n",
            "2302\n",
            "2303\n",
            "2304\n",
            "2305\n",
            "2306\n",
            "2307\n",
            "2308\n",
            "2309\n",
            "2310\n",
            "2311\n",
            "2312\n",
            "2313\n",
            "2314\n",
            "2315\n",
            "2316\n",
            "2317\n",
            "2318\n",
            "2319\n",
            "2320\n",
            "2321\n",
            "2322\n",
            "2323\n",
            "2324\n",
            "2325\n",
            "2326\n",
            "2327\n",
            "2328\n",
            "2329\n",
            "2330\n",
            "2331\n",
            "2332\n",
            "2333\n",
            "2334\n",
            "2335\n",
            "2336\n",
            "2337\n",
            "2338\n",
            "2339\n",
            "2340\n",
            "2341\n",
            "2342\n",
            "2343\n",
            "2344\n",
            "2345\n",
            "2346\n",
            "2347\n",
            "2348\n",
            "2349\n",
            "2350\n",
            "2351\n",
            "2352\n",
            "2353\n",
            "2354\n",
            "2355\n",
            "2356\n",
            "2357\n",
            "2358\n",
            "2359\n",
            "2360\n",
            "2361\n",
            "2362\n",
            "2363\n",
            "2364\n",
            "2365\n",
            "2366\n",
            "2367\n",
            "2368\n",
            "2369\n",
            "2370\n",
            "2371\n",
            "2372\n",
            "2373\n",
            "2374\n",
            "2375\n",
            "2376\n",
            "2377\n",
            "2378\n",
            "2379\n",
            "2380\n",
            "2381\n",
            "2382\n",
            "2383\n",
            "2384\n",
            "2385\n",
            "2386\n",
            "2387\n",
            "2388\n",
            "2389\n",
            "2390\n",
            "2391\n",
            "2392\n",
            "2393\n",
            "2394\n",
            "2395\n",
            "2396\n",
            "2397\n",
            "2398\n",
            "2399\n",
            "2400\n",
            "2401\n",
            "2402\n",
            "2403\n",
            "2404\n",
            "2405\n",
            "2406\n",
            "2407\n",
            "2408\n",
            "2409\n",
            "2410\n",
            "2411\n",
            "2412\n",
            "2413\n",
            "2414\n",
            "2415\n",
            "2416\n",
            "2417\n",
            "2418\n",
            "2419\n",
            "2420\n",
            "2421\n",
            "2422\n",
            "2423\n",
            "2424\n",
            "2425\n",
            "2426\n",
            "2427\n",
            "2428\n",
            "2429\n",
            "2430\n",
            "2431\n",
            "2432\n",
            "2433\n",
            "2434\n",
            "2435\n",
            "2436\n",
            "2437\n",
            "2438\n",
            "2439\n",
            "2440\n",
            "2441\n",
            "2442\n",
            "2443\n",
            "2444\n",
            "2445\n",
            "2446\n",
            "2447\n",
            "2448\n",
            "2449\n",
            "2450\n",
            "2451\n",
            "2452\n",
            "2453\n",
            "2454\n",
            "2455\n",
            "2456\n",
            "2457\n",
            "2458\n",
            "2459\n",
            "2460\n",
            "2461\n",
            "2462\n",
            "2463\n",
            "2464\n",
            "2465\n",
            "2466\n",
            "2467\n",
            "2468\n",
            "2469\n",
            "2470\n",
            "2471\n",
            "2472\n",
            "2473\n",
            "2474\n",
            "2475\n",
            "2476\n",
            "2477\n",
            "2478\n",
            "2479\n",
            "2480\n",
            "2481\n",
            "2482\n",
            "2483\n",
            "2484\n",
            "2485\n",
            "2486\n",
            "2487\n",
            "2488\n",
            "2489\n",
            "2490\n",
            "2491\n",
            "2492\n",
            "2493\n",
            "2494\n",
            "2495\n",
            "2496\n",
            "2497\n",
            "2498\n",
            "2499\n",
            "2500\n",
            "2501\n",
            "2502\n",
            "2503\n",
            "2504\n",
            "2505\n",
            "2506\n",
            "2507\n",
            "2508\n",
            "2509\n",
            "2510\n",
            "2511\n",
            "2512\n",
            "2513\n",
            "2514\n",
            "2515\n",
            "2516\n",
            "2517\n",
            "2518\n",
            "2519\n",
            "2520\n",
            "2521\n",
            "2522\n",
            "2523\n",
            "2524\n",
            "2525\n",
            "2526\n",
            "2527\n",
            "2528\n",
            "2529\n",
            "2530\n",
            "2531\n",
            "2532\n",
            "2533\n",
            "2534\n",
            "2535\n",
            "2536\n",
            "2537\n",
            "2538\n",
            "2539\n",
            "2540\n",
            "2541\n",
            "2542\n",
            "2543\n",
            "2544\n",
            "2545\n",
            "2546\n",
            "2547\n",
            "2548\n",
            "2549\n",
            "2550\n",
            "2551\n",
            "2552\n",
            "2553\n",
            "2554\n",
            "2555\n",
            "2556\n",
            "2557\n",
            "2558\n",
            "2559\n",
            "2560\n",
            "2561\n",
            "2562\n",
            "2563\n",
            "2564\n",
            "2565\n",
            "2566\n",
            "2567\n",
            "2568\n",
            "2569\n",
            "2570\n",
            "2571\n",
            "2572\n",
            "2573\n",
            "2574\n",
            "2575\n",
            "2576\n",
            "2577\n",
            "2578\n",
            "2579\n",
            "2580\n",
            "2581\n",
            "2582\n",
            "2583\n",
            "2584\n",
            "2585\n",
            "2586\n",
            "2587\n",
            "2588\n",
            "2589\n",
            "2590\n",
            "2591\n",
            "2592\n",
            "2593\n",
            "2594\n",
            "2595\n",
            "2596\n",
            "2597\n",
            "2598\n",
            "2599\n",
            "2600\n",
            "2601\n",
            "2602\n",
            "2603\n",
            "2604\n",
            "2605\n",
            "2606\n",
            "2607\n",
            "2608\n",
            "2609\n",
            "2610\n",
            "2611\n",
            "2612\n",
            "2613\n",
            "2614\n",
            "2615\n",
            "2616\n",
            "2617\n",
            "2618\n",
            "2619\n",
            "2620\n",
            "2621\n",
            "2622\n",
            "2623\n",
            "2624\n",
            "2625\n",
            "2626\n",
            "2627\n",
            "2628\n",
            "2629\n",
            "2630\n",
            "2631\n",
            "2632\n",
            "2633\n",
            "2634\n",
            "2635\n",
            "2636\n",
            "2637\n",
            "2638\n",
            "2639\n",
            "2640\n",
            "2641\n",
            "2642\n",
            "2643\n",
            "2644\n",
            "2645\n",
            "2646\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-732d8df33103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maug_data1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug_data1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-5b98daf9e4fc>\u001b[0m in \u001b[0;36mrandom_translation\u001b[0;34m(sentence, translator)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtranslations_en_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0men_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslations_en_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0men_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRANSLATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pick_service_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, params, headers, cookies, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         )\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, files, json, params, headers, cookies, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    599\u001b[0m         )\n\u001b[1;32m    600\u001b[0m         return self.send(\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         )\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         response = self.send_handling_redirects(\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m         )\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend_handling_redirects\u001b[0;34m(self, request, auth, timeout, allow_redirects, history)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             response = self.send_handling_auth(\n\u001b[0;32m--> 648\u001b[0;31m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m             )\n\u001b[1;32m    650\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend_handling_auth\u001b[0;34m(self, request, history, auth, timeout)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_flow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_response_body\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend_single_request\u001b[0;34m(self, request, timeout)\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             )\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 response = connection.request(\n\u001b[0;32m--> 153\u001b[0;31m                     \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                 )\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNewConnectionRequired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;34m\"connection.request method=%r url=%r headers=%r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         )\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimeoutDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSyncSocketStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2_stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mh2_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_streams_semaphore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# Receive the response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mreason_phrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reason_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         stream = SyncByteStream(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mreceive_response\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \"\"\"\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponseReceived\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mwait_for_event\u001b[0;34m(self, stream_id, timeout)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mreceive_events\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mRead\u001b[0m \u001b[0msome\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mH2\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n, timeout)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimeoutDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m    995\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m                     self.__class__)\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhDelQ-H6c9B"
      },
      "source": [
        "Adding random swapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4hG7jBml50k"
      },
      "source": [
        "aug_data2 = pd.DataFrame()\r\n",
        "for i,row in train_data.iterrows():\r\n",
        "    row['sentence'] = random_swap(row['sentence'].split(\" \"))\r\n",
        "    label = row['label']\r\n",
        "    aug_data2 = aug_data2.append(row).reset_index(drop=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1iS5e1pqrAs"
      },
      "source": [
        "train_data = train_data.append(aug_data).reset_index(drop=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFr5XeoluEa6"
      },
      "source": [
        "train_data = train_data.append(aug_data1).reset_index(drop=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DldvuIwmuGtu"
      },
      "source": [
        "train_data = train_data.append(aug_data2).reset_index(drop=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdaW1pJWuaTR",
        "outputId": "c2260c66-bdc4-4f18-8aad-05cd7b292d9a"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36242, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUFffVIOuIL9"
      },
      "source": [
        "train_data.drop_duplicates(inplace=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXH4xB-wuUj6",
        "outputId": "1fd8c72e-63d5-4541-b3e3-228de93b094f"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25102, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6o_79ISSVb"
      },
      "source": [
        "## Defining Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e63g08ijOrf7"
      },
      "source": [
        "Now we shall be defining LABEL as a LabelField, which is a subclass of Field that sets sequen tial to False (as it’s our numerical category class). TWEET is a standard Field object, where we have decided to use the spaCy tokenizer and convert all the text to lower‐ case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk8IP4SK1Lrp",
        "outputId": "a9362af6-23d8-411a-8214-cbb56ad202ce"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext import data \n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa8ca01ff30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6bKQax2Mf_U"
      },
      "source": [
        "Tweet = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX-lYIe_O7Vy"
      },
      "source": [
        "Having defined those fields, we now need to produce a list that maps them onto the list of rows that are in the CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VawdWq36O6td"
      },
      "source": [
        "fields = [('tweets', Tweet),('labels',Label)]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgS6G6WLvLcc"
      },
      "source": [
        "valid_data=valid_data.reset_index(drop=True)\r\n",
        "train_data=train_data.reset_index(drop=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbtZ-Ph2P1xL"
      },
      "source": [
        "Armed with our declared fields, lets convert from pandas to list to torchtext. We could also use TabularDataset to apply that definition to the CSV directly but showing an alternative approach too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3OLcJ5B7rHz"
      },
      "source": [
        "train_example = [data.Example.fromlist([train_data.sentence[i],train_data.label[i]], fields) for i in range(train_data.shape[0])] \r\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIOuUI4dvTX-"
      },
      "source": [
        "valid_example = [data.Example.fromlist([valid_data.sentence[i],valid_data.label[i]], fields) for i in range(valid_data.shape[0])]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT-flpH-P1cd"
      },
      "source": [
        "# Creating dataset\n",
        "#twitterDataset = data.TabularDataset(path=\"tweets.csv\", format=\"CSV\", fields=fields, skip_header=True)\n",
        "\n",
        "train = data.Dataset(train_example, fields)\n",
        "valid = data.Dataset(valid_example, fields)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ZnyCPaR08F"
      },
      "source": [
        "Finally, we can split into training, testing, and validation sets by using the split() method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykvsCGQMR6UD",
        "outputId": "4537520c-4525-433c-9238-61284b134e20"
      },
      "source": [
        "(len(train), len(valid))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25102, 3311)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kix8P2IKSBaV"
      },
      "source": [
        "An example from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUpEOQruR9JL",
        "outputId": "35310694-3e37-4f65-bad3-52d79e598129"
      },
      "source": [
        "vars(train.examples[10])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 4.0,\n",
              " 'tweets': ['Good',\n",
              "  'fun',\n",
              "  ',',\n",
              "  'good',\n",
              "  'action',\n",
              "  ',',\n",
              "  'good',\n",
              "  'acting',\n",
              "  ',',\n",
              "  'good',\n",
              "  'dialogue',\n",
              "  ',',\n",
              "  'good',\n",
              "  'pace',\n",
              "  ',',\n",
              "  'good',\n",
              "  'cinematography',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKdllP3FST4N"
      },
      "source": [
        "## Building Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuvWQ-SpSmSz"
      },
      "source": [
        "At this point we would have built a one-hot encoding of each word that is present in the dataset—a rather tedious process. Thankfully, torchtext will do this for us, and will also allow a max_size parameter to be passed in to limit the vocabu‐ lary to the most common words. This is normally done to prevent the construction of a huge, memory-hungry model. We don’t want our GPUs too overwhelmed, after all. \n",
        "\n",
        "Let’s limit the vocabulary to a maximum of 5000 words in our training set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx955u93SGeY"
      },
      "source": [
        "Tweet.build_vocab(train)\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvyEeEjXTGhX"
      },
      "source": [
        "By default, torchtext will add two more special tokens, <unk> for unknown words and <pad>, a padding token that will be used to pad all our text to roughly the same size to help with efficient batching on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA3tIESdcJdN",
        "outputId": "bbc6bee5-a8a2-4db0-d935-70f321ae5196"
      },
      "source": [
        "print('Size of input vocab : ', len(Tweet.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Tweet.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  17212\n",
            "Size of label vocab :  14\n",
            "Top 10 words appreared repeatedly : [('.', 22073), (',', 19845), ('the', 16830), ('and', 12381), ('of', 12357), ('a', 12275), ('to', 8442), ('-', 7565), (\"'s\", 7075), ('is', 7017)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7fa87ad74488>, {0.0: 0, 1.0: 1, 3.0: 2, 2.0: 3, 7.0: 4, 4.0: 5, 6.0: 6, 5.0: 7, 10.0: 8, 9.0: 9, 8.0: 10, 12.0: 11, 11.0: 12, 13.0: 13})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwjD2-ebTeUX"
      },
      "source": [
        "**Lots of stopwords!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLWW221gTpNs"
      },
      "source": [
        "Now we need to create a data loader to feed into our training loop. Torchtext provides the BucketIterator method that will produce what it calls a Batch, which is almost, but not quite, like the data loader we used on images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQqMhMoDUDmn"
      },
      "source": [
        "But at first declare the device we are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfo2QhGJUK4l"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK2ORoqdTNsM"
      },
      "source": [
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid), batch_size = 32, \n",
        "                                                            sort_key = lambda x: len(x.tweets),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg7gTFQO4fby"
      },
      "source": [
        "Save the vocabulary for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niE9Cc6-2bD_"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Tweet.vocab.stoi, tokens)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AbsQwqkVyAy"
      },
      "source": [
        "## Defining Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PED4HJWH4t"
      },
      "source": [
        "We use the Embedding and LSTM modules in PyTorch to build a simple model for classifying tweets.\n",
        "\n",
        "In this model we create three layers. \n",
        "1. First, the words in our tweets are pushed into an Embedding layer, which we have established as a 300-dimensional vector embedding. \n",
        "2. That’s then fed into a 2 stacked-LSTMs with 100 hidden features (again, we’re compressing down from the 300-dimensional input like we did with images). We are using 2 LSTMs for using the dropout.\n",
        "3. Finally, the output of the LSTM (the final hidden state after processing the incoming tweet) is pushed through a standard fully connected layer with three outputs to correspond to our three possible classes (negative, positive, or neutral)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43pVRccMT0bT"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwBoGE_X_Fl8"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Tweet.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 100\n",
        "num_output_nodes = 14\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-pOMqzJ3eTv",
        "outputId": "c47b9083-a1ad-4fb2-8c69-a2d176fbc4e9"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(17212, 300)\n",
            "  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (fc): Linear(in_features=100, out_features=14, bias=True)\n",
            ")\n",
            "The model has 5,406,614 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXajorf5Xz7t"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrE9RpMtZ1Vs"
      },
      "source": [
        "First define the optimizer and loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u86JWdlXvu5"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VCJtNb3Zt8w"
      },
      "source": [
        "The main thing to be aware of in this new training loop is that we have to reference `batch.tweets` and `batch.labels` to get the particular fields we’re interested in; they don’t fall out quite as nicely from the enumerator as they do in torchvision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WjEPLKsAiS_"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDWNnGK3Y5oJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        tweet, tweet_lengths = batch.tweets   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(tweet, tweet_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZcHhkkvAsCt"
      },
      "source": [
        "**Evaluation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHEe-zSVAriL"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            tweet, tweet_lengths = batch.tweets\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(tweet, tweet_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            acc = binary_accuracy(predictions, batch.labels)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6LJFW7HaJoV"
      },
      "source": [
        "**Let's Train and Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq330XlnaEU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f64bd93-729a-4a45-92e1-a0f6daa1c3ce"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 2.230 | Train Acc: 55.01%\n",
            "\t Val. Loss: 2.196 |  Val. Acc: 55.94% \n",
            "\n",
            "\tTrain Loss: 2.191 | Train Acc: 56.37%\n",
            "\t Val. Loss: 2.195 |  Val. Acc: 55.94% \n",
            "\n",
            "\tTrain Loss: 2.191 | Train Acc: 56.39%\n",
            "\t Val. Loss: 2.196 |  Val. Acc: 55.94% \n",
            "\n",
            "\tTrain Loss: 2.190 | Train Acc: 56.56%\n",
            "\t Val. Loss: 2.196 |  Val. Acc: 55.94% \n",
            "\n",
            "\tTrain Loss: 2.184 | Train Acc: 57.25%\n",
            "\t Val. Loss: 2.191 |  Val. Acc: 56.36% \n",
            "\n",
            "\tTrain Loss: 2.152 | Train Acc: 60.67%\n",
            "\t Val. Loss: 2.184 |  Val. Acc: 57.02% \n",
            "\n",
            "\tTrain Loss: 2.111 | Train Acc: 64.69%\n",
            "\t Val. Loss: 2.175 |  Val. Acc: 58.07% \n",
            "\n",
            "\tTrain Loss: 2.083 | Train Acc: 67.57%\n",
            "\t Val. Loss: 2.172 |  Val. Acc: 58.10% \n",
            "\n",
            "\tTrain Loss: 2.056 | Train Acc: 70.20%\n",
            "\t Val. Loss: 2.170 |  Val. Acc: 58.43% \n",
            "\n",
            "\tTrain Loss: 2.032 | Train Acc: 72.67%\n",
            "\t Val. Loss: 2.158 |  Val. Acc: 59.24% \n",
            "\n",
            "\tTrain Loss: 2.011 | Train Acc: 74.79%\n",
            "\t Val. Loss: 2.160 |  Val. Acc: 59.12% \n",
            "\n",
            "\tTrain Loss: 1.996 | Train Acc: 76.11%\n",
            "\t Val. Loss: 2.157 |  Val. Acc: 59.33% \n",
            "\n",
            "\tTrain Loss: 1.987 | Train Acc: 77.04%\n",
            "\t Val. Loss: 2.151 |  Val. Acc: 60.29% \n",
            "\n",
            "\tTrain Loss: 1.976 | Train Acc: 78.17%\n",
            "\t Val. Loss: 2.154 |  Val. Acc: 60.11% \n",
            "\n",
            "\tTrain Loss: 1.967 | Train Acc: 79.01%\n",
            "\t Val. Loss: 2.154 |  Val. Acc: 60.05% \n",
            "\n",
            "\tTrain Loss: 1.960 | Train Acc: 79.67%\n",
            "\t Val. Loss: 2.143 |  Val. Acc: 60.83% \n",
            "\n",
            "\tTrain Loss: 1.953 | Train Acc: 80.35%\n",
            "\t Val. Loss: 2.155 |  Val. Acc: 59.81% \n",
            "\n",
            "\tTrain Loss: 1.946 | Train Acc: 81.23%\n",
            "\t Val. Loss: 2.148 |  Val. Acc: 60.87% \n",
            "\n",
            "\tTrain Loss: 1.935 | Train Acc: 82.49%\n",
            "\t Val. Loss: 2.149 |  Val. Acc: 60.86% \n",
            "\n",
            "\tTrain Loss: 1.927 | Train Acc: 83.36%\n",
            "\t Val. Loss: 2.168 |  Val. Acc: 58.75% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}